#!/usr/bin/env python3

# A personal script to import papers (PDF) into personal dropbox folder.


import dataclasses
import os
import re
import shlex
import shutil
import subprocess as sp
import urllib.request
from datetime import datetime
from distutils.util import strtobool as str2bool
from pathlib import Path
from typing import List, Optional, Union
from urllib.parse import urlparse

# pip install arxiv blessed openreview-py
import arxiv
import blessed
import openreview

term = blessed.Terminal()


PDF_BASE_PATH = "~/Dropbox/papers/articles"
NOTE_BASE_PATH = "~/Dropbox/Notable/notes"

NOTE_TEMPLATE = """\
---
title: '{title}'
created: '{date}'
modified: '{date}'
tags: []
---

# {title}

{authors}
{url}
"""

def clean_path(title: str) -> str:
    title = title.replace('/', '')
    title = title.replace("'", '')
    title = title.replace('"', '')
    title = title.replace('@', ' ')
    title = re.sub(r'\s*:\s*', ' - ', title)
    title = re.sub(r'[^\w\-_\. ]', '_', title)
    return title


ansi_escape = re.compile(r'(?:\x1B[@-_]|[\x80-\x9F])[0-?]*[ -/]*[@-~]')

def log_boxed(msg, len_adjust=0):
    import unicodedata
    w = 0
    for line in msg.split('\n'):
        l = sum(not unicodedata.combining(ch) for ch in ansi_escape.sub('', line)) + len_adjust
        w = max(w, l)

    print("┏" + ("━" * w) + "┓\n", end='')
    for line in msg.split('\n'):
        l = sum(not unicodedata.combining(ch) for ch in ansi_escape.sub('', line)) + len_adjust
        if not line:
            continue
        print("┃" + line   , end='')
        print(" " * (w - l + 3) + "┃\n", end='')
    print("┗" + ("━" * w) + "┛\n", end='')


@dataclasses.dataclass
class PaperEntry:
    url: str
    pdf_url: str
    title: str
    authors: List[str]
    publication_year: Union[int, str]


def handle_arxiv(url: str) -> Optional[PaperEntry]:
    if 'arxiv.org' not in url:
        return None

    url = re.sub(pattern=r'^https?://arxiv.org/pdf/([0-9.]+)(v\d+)?(\.pdf)?$',
                 repl=r'https://arxiv.org/abs/\1', string=url)

    arxiv_query = os.path.basename(url).rstrip('.pdf')
    ret = list(arxiv.Search(arxiv_query, max_results=1).results())
    if not ret:
        raise RuntimeError(f"No result for {arxiv_query}")
    entry = ret[0]   # type: ignore
    assert arxiv_query in entry.pdf_url, (
        f"entry = {entry} did not match {arxiv_query}")

    return PaperEntry(
        url=url,
        title=re.sub(r'[\s]+', ' ', entry.title),
        authors=[a.name for a in entry.authors],
        publication_year=entry.published and entry.published.year or 'Unknown',  # type: ignore
        pdf_url=entry.pdf_url,
    )


def handle_openreview(url: str) -> Optional[PaperEntry]:
    if 'openreview.net' not in url:
        return None

    url = re.sub(pattern=r'^https?://openreview.net/pdf\?id=(\w+)$',
                 repl=r'https://openreview.net/forum?id=\1', string=url)

    id = re.sub(pattern=r'^https?://openreview.net/(pdf|forum)\?id=(\w+)$',
                repl=r'\2', string=url)

    client = openreview.Client(baseurl='https://api.openreview.net')
    note = client.get_note(id)
    m_year = re.match('(NeurIPS.cc|ICLR.cc)/([0-9]+)', note.invitation)
    year = m_year and int(m_year.group(2)) or 'Unknown'

    return PaperEntry(
        url=url,
        title=note.content['title'],
        authors=list(note.content['authors']),
        publication_year=year,
        pdf_url=f'https://openreview.net/pdf?id={id}',
    )


def manual_input(url: str) -> PaperEntry:
    title = input("Title> ").strip()
    authors = input("Authors (comma-sep)> ")
    authors = [x.strip() for x in authors.split(',')]
    publication_year = int(input("Year> ").strip())
    pdf_url = url
    assert title
    assert authors
    return PaperEntry(url=url, title=title, authors=authors,
                      publication_year=publication_year, pdf_url=pdf_url)


def import_paper(url: str, *, force=False,
                 write_notable=True,
                 import_into_papers=True,
                 ):
    if not isinstance(url, str):
        raise TypeError("`url` must be a {}, but given {}".format(str, type(url)))
    if urlparse(url).scheme not in ('http', 'https'):
        raise ValueError("Not an url: {}".format(url))

    # Detect arxiv or openreview PDF files, and canonize URL
    url = re.sub(pattern=r'^https?://arxiv.org/pdf/([0-9.]+)(v\d+)?(\.pdf)?$',
                 repl=r'https://arxiv.org/abs/\1', string=url)
    print(f"Importing {url} ...")

    entry = None
    entry = entry or handle_arxiv(url)
    entry = entry or handle_openreview(url)
    entry = entry or manual_input(url)

    msg = ''
    msg += ("  " + term.bold_green(entry.title) + '\n')
    msg += ("  " + term.yellow(', '.join(entry.authors)) + '\n')
    msg += ("  " + term.cyan(url) + '\n')
    log_boxed(msg)

    # notable
    md_file = os.path.join(os.path.expanduser(NOTE_BASE_PATH),
                           clean_path(entry.title) + ".md")
    date_iso = datetime.now().astimezone().isoformat()  # local timezone
    if write_notable:
        if not os.path.exists(md_file) or force:
            # Create a backup
            if force and os.path.exists(md_file):
                backup_path = re.sub(".md$", ".backup.md", str(md_file))
                shutil.copy2(md_file, backup_path)
                print(term.yellow(f"Created a backup of note at {backup_path}"))

            # Write a new note file
            with open(md_file, "wt") as fp:
                print(f"Writing to {md_file} ...")
                fp.write(NOTE_TEMPLATE.format(
                    title=entry.title, date=date_iso,
                    authors=', '.join(entry.authors), url=entry.url))
        else:
            print(term.yellow(f"Already exists: {md_file}"))

        # Open .md with the default editor (Typora)
        sp.call("cd {} && open {}".format(
            shlex.quote(os.path.dirname(md_file)),
            shlex.quote(md_file)
        ), shell=True)

    # papers
    if import_into_papers:
        print("Importing {} into Library ...".format(url))
        #cmd = "open -a Papers {}".format(shlex.quote(url))
        #sp.call(cmd, shell=True)

        # TODO: This may not work if the author format is not compatible.
        first_author_lastname = entry.authors[0].split()[-1]

        base_path = Path(PDF_BASE_PATH).expanduser()
        base_path = base_path / str(entry.publication_year)
        base_path.mkdir(exist_ok=True)

        pdf_path = base_path / f'{first_author_lastname} {entry.publication_year} - {entry.title}.pdf'
        if force or not pdf_path.exists():
            # Create a backup
            if force and pdf_path.exists():
                backup_path = re.sub(".pdf$", ".backup.pdf", str(pdf_path))
                shutil.copy2(pdf_path, backup_path)
                print(term.yellow(f"[!] Created a backup of PDF at {backup_path}"))

            # Download the PDF file
            urllib.request.urlretrieve(entry.pdf_url, str(pdf_path))
            print(f"Downloaded {entry.pdf_url} into:\n'{pdf_path}'")
        else:
            print(term.yellow(f"Skipped (already exists): {pdf_path}"))

        # Open the PDF file.
        sp.call("open {}".format(shlex.quote(str(pdf_path))), shell=True)


def main():
    import argparse
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('url', nargs='+')
    parser.add_argument('--force', '-f', default=False, action='store_true')
    parser.add_argument('--import-into-papers', '-p', default=True, type=str2bool)
    parser.add_argument('--no-papers', dest='import_into_papers', action='store_false')
    parser.add_argument('--write-notable', '-n', default=True, type=str2bool)
    parser.add_argument('--no-notable', dest='write_notable', action='store_false')
    args = parser.parse_args()

    for url in args.url:
        import_paper(url=url, force=args.force,
                     import_into_papers=args.import_into_papers,
                     write_notable=args.write_notable)

if __name__ == '__main__':
    main()
